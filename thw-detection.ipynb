{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting terrestrial heatwaves in Australia  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to take in daily tmax data, along with previously computed mean climatology and threshold values, process these and output all the THW events that occur in the dataset\n",
    "\n",
    "First, we import the data and required modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages and defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "import dask.array\n",
    "import datetime\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "from scipy.ndimage.measurements import label, find_objects\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PBS_NCPUS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a3624a3d0f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     c = dask.distributed.Client(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mn_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PBS_NCPUS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mthreads_per_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mthreads_per_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads_per_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmemory_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{4*threads_per_worker}gb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.10/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PBS_NCPUS'"
     ]
    }
   ],
   "source": [
    "# This is for running on Gadi\n",
    "\n",
    "import os\n",
    "import dask.distributed\n",
    "threads_per_worker = 1\n",
    "try:\n",
    "    c # Already running\n",
    "except NameError:\n",
    "    c = dask.distributed.Client(\n",
    "        n_workers=int(os.environ['PBS_NCPUS'])//threads_per_worker,\n",
    "        threads_per_worker=threads_per_worker,\n",
    "        memory_limit=f'{4*threads_per_worker}gb',\n",
    "        local_directory=os.path.join(os.environ['PBS_JOBFS'],\n",
    "                                     'dask-worker-space')\n",
    "    )\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will make the clim and thresh repeatable so that the shapes of clim, thresh and obs are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(ds):\n",
    "    trial = ds\n",
    "    i = 0\n",
    "    while i < 37:  # <-- Here, 37 corresponds to the number of years in the observation dataset \n",
    "        trial = xr.concat([trial, ds], 'dayofyear')\n",
    "        i+=1\n",
    "    trial = trial.isel(dayofyear = slice(0,13514))\n",
    "    # to specify the dates/time \n",
    "    trial.coords['dayofyear'] = np.arange(date(1982,1,1).toordinal(),date(2018,12,31).toordinal()+1) \n",
    "    \n",
    "    # This code was used to rename the dayofyear dimension to time.\n",
    "    trial['time'] = trial['dayofyear']\n",
    "    del trial['dayofyear']\n",
    "    trial = trial.rename({'dayofyear': 'time'})\n",
    "    \n",
    "    t = np.arange(date(1982,1,1).toordinal(),date(2018,12,31).toordinal()+1)\n",
    "    dates = [date.fromordinal(tt.astype(int)) for tt in t]\n",
    "    \n",
    "    trial.coords['time'] = dates\n",
    "\n",
    "    return trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'calc_severity()' calculates severity based on the severity index introduced by Hobday et al (2018) for categorizing marine heatwaves.\n",
    "\n",
    "This function merges together the chunks along the time dimension by setting the 'time' chunk size to None, so that the resulting chunks each contain the full time series for that location. To run the severity calculaion on each timeseries chunk, '.map_blocks()' is used so that the calculation can be done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_severity(data, new_climatology, new_threshold):\n",
    "\n",
    "    threshold_anomaly = new_threshold - new_climatology\n",
    "    \n",
    "    # Ignore divide by zero errors\n",
    "    nperr = np.seterr(divide='ignore')\n",
    "\n",
    "    def calc_severity_helper(da, *args, **kwargs):\n",
    "        if da.size == 0:\n",
    "            return da\n",
    "    \n",
    "        coords = {}\n",
    "        for k,v in obs_aus_tmax.coords.items():\n",
    "            if k == 'time':\n",
    "                continue\n",
    "            coords[k] = slice(v[0], v[-1])\n",
    "    \n",
    "        anomaly = da - new_climatology.sel(coords)\n",
    "        severity = anomaly / threshold_anomaly.sel(coords)\n",
    "        return severity\n",
    "\n",
    "    data = data.chunk({'time': None})\n",
    "    r = data.map_blocks(calc_severity_helper)\n",
    "    \n",
    "    np.seterr(**nperr)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns values with at least n contiguous points around them. I am using n=3, to define a terrestrial heatwave as an event where the daily tmax exceeds the 90th percentile for at least 3 consecutive days (Perkins and Alexander, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atleastn(da, n, dim='time'):\n",
    "\n",
    "    def atleastn_helper(array, n, axis):\n",
    "        count = np.zeros_like(np.take(array, 0,axis=axis), dtype='i4')\n",
    "        mask = np.empty_like(np.take(array, 0,axis=axis), dtype='bool')\n",
    "        mask = True\n",
    "    \n",
    "        for i in range(array.shape[axis]):\n",
    "            array_slice = np.take(array, i, axis=axis)\n",
    "        \n",
    "            # Increase the count when there is a valid value, reset when there is not\n",
    "            # This was initially set to 0, now I have changed it to 1 to detect only valid heatwave days \n",
    "            # The previous way was fine as long as I masked values less than or equal to 1, and they were white on the colour bar\n",
    "            count = np.where(array_slice > 1, count + 1, 0)\n",
    "        \n",
    "            # Add new points when the contiguous count exceeds the threshold\n",
    "            mask = np.where(count >= n, False, mask)\n",
    "            \n",
    "        out_slice = np.take(array, array.shape[axis]//2, axis=axis)\n",
    "        return np.where(mask, np.nan, out_slice)\n",
    "    \n",
    "    def atleastn_dask_helper(array, axis, **kwargs):\n",
    "        r = dask.array.map_blocks(atleastn_helper, array, drop_axis=axis, axis=axis, n=n, dtype=array.dtype)\n",
    "        return r\n",
    "    \n",
    "    if isinstance(da.data, dask.array.Array):\n",
    "        reducer = atleastn_dask_helper\n",
    "    else:\n",
    "        reducer = atleastn_helper\n",
    "        \n",
    "    return da.rolling({dim: n*2-1}, center=True, min_periods=n).reduce(reducer, n=n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening files and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_climatology = xr.open_dataarray('/g/data/e14/cp3790/Charuni/climatology-australia.nc')\n",
    "threshold = xr.open_dataarray('/g/data/e14/cp3790/Charuni/threshold-australia.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('/g/data/e14/cp3790/Charuni/ERA5-new/era5_dailytmax_*.nc'))\n",
    "\n",
    "obs_aus = (xr.open_mfdataset(files, combine='nested', concat_dim='time', chunks={'latitude': 10})\n",
    "           .sel(time=slice('1982', '2018'), longitude=slice(113, 154), latitude=slice(-10, -44)))\n",
    "obs_aus_tmax = obs_aus[\"dmax\"]\n",
    "obs_aus_tmax.attrs['units'] = 'deg C'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the mean climatology and 90th percentile climatology (threshold) repeatable so that their sizes are compatible with the size of the daily tmax data using the 'fix' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_climatology = fix(mean_climatology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold = fix(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating severity using 'map_blocks()', which enables the severity calculation to run on each timeseries chunk, facilitating parallel computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "thw_severity_chai = calc_severity(obs_aus_tmax, new_climatology, new_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the days where the daily tmax has exceeded the threshold. If severity > 1, this means that daily tmax > threshold (from the definition of severity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "\n",
    "candidates = thw_severity_chai.where(thw_severity_chai > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we mask out points where there are less than 3 contiguous points in the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53 ms, sys: 3 ms, total: 56 ms\n",
      "Wall time: 54.1 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray (time: 13514, latitude: 137, longitude: 165)&gt;\n",
       "dask.array&lt;where, shape=(13514, 137, 165), dtype=float32, chunksize=(13514, 10, 165), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 -10.0 -10.25 -10.5 ... -43.5 -43.75 -44.0\n",
       "  * time       (time) datetime64[ns] 1982-01-01 1982-01-02 ... 2018-12-31\n",
       "  * longitude  (longitude) float64 113.0 113.2 113.5 113.8 ... 153.5 153.8 154.0</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray (time: 13514, latitude: 137, longitude: 165)>\n",
       "dask.array<where, shape=(13514, 137, 165), dtype=float32, chunksize=(13514, 10, 165), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 -10.0 -10.25 -10.5 ... -43.5 -43.75 -44.0\n",
       "  * time       (time) datetime64[ns] 1982-01-01 1982-01-02 ... 2018-12-31\n",
       "  * longitude  (longitude) float64 113.0 113.2 113.5 113.8 ... 153.5 153.8 154.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "atleastn(candidates, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test on 1D array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray (time: 20)&gt;\n",
       "array([ 3.,  1.,  4., nan, nan, nan, nan, nan,  1.,  1.,  3., nan, nan,\n",
       "       nan, nan,  1.,  2.,  4.,  2., nan])\n",
       "Dimensions without coordinates: time</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray (time: 20)>\n",
       "array([ 3.,  1.,  4., nan, nan, nan, nan, nan,  1.,  1.,  3., nan, nan,\n",
       "       nan, nan,  1.,  2.,  4.,  2., nan])\n",
       "Dimensions without coordinates: time"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = xr.DataArray([3, 1, 4, 0, 0, 1, 4, 0, 1, 1, 3, 0, 2, 4, 0, 1, 2, 4, 2, 0], dims=['time'])\n",
    "\n",
    "atleastn(test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to netCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the terrestrial heatwave days across Australia from 1982-2018 will be stored in filtered_severity_final_new_mask.nc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data3/hh5/public/apps/miniconda3/envs/analysis3-19.10/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 8min 5s, total: 10min 45s\n",
      "Wall time: 11min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "oscar = atleastn(candidates, n=3)\n",
    "\n",
    "xr.Dataset({'severity': oscar}).to_netcdf('/g/data/e14/cp3790/Charuni/filtered_severity_final_new_mask.nc',\n",
    "                                              encoding={'severity': \n",
    "                                                        {'chunksizes': (100, oscar.shape[1], oscar.shape[2]),\n",
    "                                                         'zlib': True,\n",
    "                                                         'shuffle': True, \n",
    "                                                         'complevel': 2}})   \n",
    "\n",
    "# compression level (complevel) up to 6 is fine, >6 and it starts giving trouble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "305.85px",
    "left": "1531px",
    "right": "20px",
    "top": "120px",
    "width": "349px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
